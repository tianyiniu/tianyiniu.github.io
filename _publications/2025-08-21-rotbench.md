---
title: "RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation"
collection: publications
category: manuscripts
permalink: /publication/2025-08-21-rotbench
excerpt: ''
date: 2025-08-21
venue: 'Arxiv preprint'
# slidesurl: 'http://academicpages.github.io/files/slides1.pdf'
# paperurl: 'http://academicpages.github.io/files/paper1.pdf'
# bibtexurl: 'http://academicpages.github.io/files/bibtex1.bib'
# citation: 'Your Name, You. (2009). &quot;Paper Title Number 1.&quot; <i>Journal 1</i>. 1(1).'
---

**Authors**

<u>Tianyi Niu</u>, Jaemin Cho, Elias Stengel-Eskin, Mohit Bansal

**Abstract**

We investigate to what extent Multimodal Large Language Models (MLLMs) can accurately identify the orientation of input images rotated 0°, 90°, 180°, and 270°. This task demands robust visual reasoning capabilities to detect rotational cues and contextualize spatial relationships within images, regardless of their orientation. To evaluate MLLMs on these abilities, we introduce \rotbench{} -- a 350-image manually-filtered benchmark comprising lifestyle, portrait, and landscape images. Despite the relatively simple nature of this task, we show that several state-of-the-art open and proprietary MLLMs, including GPT-5, o3, and Gemini-2.5-Pro, do not reliably identify rotation in input images. Providing models with auxiliary information -- including captions, depth maps, and more -- or using chain-of-thought prompting offers only small and inconsistent improvements. Our results indicate that most models are able to reliably identify right-side-up (0°) images, while certain models are able to identify upside-down (180°) images. None can reliably distinguish between 90° and 270°. Simultaneously showing the image rotated in different orientations leads to moderate performance gains for reasoning models, while a modified setup using voting improves the performance of weaker models. We further show that fine-tuning does not improve models' ability to distinguish 90° and 270° rotations, despite substantially improving the identification of 180° images. Together, these results reveal a significant gap between MLLMs' spatial reasoning capabilities and human perception in identifying rotation.

**Paper**: [https://arxiv.org/abs/2508.13968](https://arxiv.org/abs/2508.13968)

**Code and Data**: [https://github.com/tianyiniu/RotBench](https://github.com/tianyiniu/RotBench)

**Huggingface**: [https://huggingface.co/papers/2508.13968](https://huggingface.co/papers/2508.13968)